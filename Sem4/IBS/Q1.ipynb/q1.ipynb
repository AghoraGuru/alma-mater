{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Forward Algorithm\n",
    "The forward algorithm is a dynamic programming algorithm that calculates the probability of observing a sequence of observations given a hidden Markov model (HMM). An HMM is a statistical model that consists of a set of states and a set of observations. The states are connected by transition probabilities, and each state emits observations with certain probabilities.\n",
    "\n",
    "The forward algorithm starts by initializing the forward probabilities for the first observation, which is the product of the starting probability of each state and the emission probability of that state for the first observation. Then, it iterates over the sequence of observations and calculates the forward probabilities for each time step. Each forward probability is the sum of the product of the previous forward probabilities for each possible previous state, the transition probability from that state to the current state, and the emission probability of the current state for the current observation. Finally, the algorithm returns the forward probabilities for the last time step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Model (HMM)\n",
    "A Hidden Markov Model (HMM) is a statistical model used for modeling time series data that contains hidden (or unobserved) states. It is a type of Markov chain where each state in the chain is associated with a probability distribution over possible observations. HMMs are widely used in speech recognition, natural language processing, and bioinformatics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Forward Algorithm\n",
    "Here is the implementation of the forward algorithm in Python. This algorithm is used to calculate the probability of observing a sequence of observations given a hidden Markov model (HMM).\n",
    "\n",
    "```py\n",
    "def calculate_forward_probabilities(observations, states, start_probabilities, transition_probabilities, emission_probabilities):\n",
    "  \"\"\"\n",
    "  Calculates the forward probabilities for the given sequence of observations.\n",
    "  Returns: A list of forward probabilities, one for each time step.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize the forward probabilities.\n",
    "  forward_probabilities = []\n",
    "  for state in states:\n",
    "    forward_probabilities.append(start_probabilities[state] * emission_probabilities[state][observations[0]])\n",
    "\n",
    "  # Iterate over the sequence of observations.\n",
    "  for t in range(1, len(observations)):\n",
    "    new_forward_probabilities = []\n",
    "    for state in states:\n",
    "      new_forward_probabilities.append(\n",
    "        sum(\n",
    "          forward_probabilities[t - 1][prev_state] * transition_probabilities[prev_state][state] * emission_probabilities[state][observations[t]]\n",
    "          for prev_state in states\n",
    "        )\n",
    "      )\n",
    "    forward_probabilities = new_forward_probabilities\n",
    "\n",
    "  # Return the forward probabilities.\n",
    "  return forward_probabilities\n",
    "\n",
    "\n",
    "def calculate_probability_of_observations(observations, states, start_probabilities, transition_probabilities, emission_probabilities):\n",
    "  \"\"\"\n",
    "  Calculates the probability of observing the given sequence of observations.\n",
    "  \"\"\"\n",
    "  forward_probabilities = calculate_forward_probabilities(observations, states, start_probabilities, transition_probabilities, emission_probabilities)\n",
    "  return forward_probabilities[-1][states[-1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033418500000000004\n"
     ]
    }
   ],
   "source": [
    "obs = ['A', 'B', 'A', 'A']\n",
    "states = ['S1', 'S2']\n",
    "start_prob = {'S1': 0.6, 'S2': 0.4}\n",
    "trans_prob = {'S1': {'S1': 0.7, 'S2': 0.3}, 'S2': {'S1': 0.4, 'S2': 0.6}}\n",
    "emit_prob = {'S1': {'A': 0.1, 'B': 0.9}, 'S2': {'A': 0.8, 'B': 0.2}}\n",
    "\n",
    "prob = forward_algorithm(obs, states, start_prob, trans_prob, emit_prob)\n",
    "print(prob)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "The Viterbi algorithm is used to find the most likely sequence of states that produced a given sequence of emissions. Here's an implementation of the Viterbi algorithm in Python:\n",
    "\n",
    "```py\n",
    "def find_most_likely_sequence(observations, states, start_probs, transition_probs, emission_probs):\n",
    "    # Get the number of observations and states\n",
    "    num_observations = len(observations)\n",
    "    num_states = len(states)\n",
    "    \n",
    "    # Initialize the Viterbi matrix and the backpointer matrix\n",
    "    viterbi = [[0.0] * num_states for _ in range(num_observations)]\n",
    "    backpointers = [[0] * num_states for _ in range(num_observations)]\n",
    "    \n",
    "    # Initialize the first column of the Viterbi matrix\n",
    "    for state in range(num_states):\n",
    "        viterbi[0][state] = start_probs[state] * emission_probs[state][observations[0]]\n",
    "    \n",
    "    # Compute the Viterbi values and backpointers for each observation\n",
    "    for t in range(1, num_observations):\n",
    "        for state in range(num_states):\n",
    "            max_prob = 0.0\n",
    "            max_prev_state = 0\n",
    "            for prev_state in range(num_states):\n",
    "                prob = viterbi[t-1][prev_state] * transition_probs[prev_state][state]\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_prev_state = prev_state\n",
    "            viterbi[t][state] = max_prob * emission_probs[state][observations[t]]\n",
    "            backpointers[t][state] = max_prev_state\n",
    "    \n",
    "    # Find the sequence of states with the highest probability\n",
    "    sequence = []\n",
    "    max_prob = max(viterbi[num_observations-1])\n",
    "    max_state = viterbi[num_observations-1].index(max_prob)\n",
    "    sequence.append(max_state)\n",
    "    \n",
    "    for t in range(num_observations-1, 0, -1):\n",
    "        max_state = backpointers[t][max_state]\n",
    "        sequence.insert(0, max_state)\n",
    "    \n",
    "    return sequence\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations = [0, 1, 3]  # U=0, N=1, Rc=3\n",
    "states = [0, 1]  # S=0, R=1\n",
    "start_prob = [0.6, 0.4]\n",
    "transition_prob = [[0.7, 0.3],\n",
    "                   [0.4, 0.6]]\n",
    "emission_prob = [[0.1, 0.9, 0.0, 0.0],\n",
    "                 [0.0, 0.2, 0.0, 0.8]]\n",
    "find_most_likely_sequence(observations, states, start_prob, transition_prob, emission_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
