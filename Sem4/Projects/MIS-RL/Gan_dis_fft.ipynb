{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 03:48:07.746680: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-03 03:48:09.478236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Convert images to frequency domain using 2D FFT\n",
    "def preprocess_images(images):\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        freq_img = (np.fft.fft2(img))\n",
    "        #taking only the real part of the frequency image\n",
    "        freq_img = np.real(freq_img)\n",
    "        processed_images.append(freq_img)\n",
    "    return np.array(processed_images)\n",
    "\n",
    "# Load the dataset\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Rescale -1 to 1 and preprocess images\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_train = preprocess_images(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 03:07:04.185172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 03:07:04.317896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 03:07:04.318265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 03:07:04.321211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 03:07:04.322217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 03:07:04.322689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 03:07:05.634023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 03:07:05.634424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 03:07:05.634676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 03:07:05.634888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2584 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 256)               0         \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 256)               25856     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 1024)              0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 784)               803600    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, BatchNormalization, LeakyReLU, Lambda\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class GAN():\n",
    "    def __init__(self, img_rows, img_cols, channels, latent_dim):\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(28 * 28, activation='tanh'))  # Output shape: (None, 28*28)\n",
    "        model.add(Reshape((28, 28, 1)))  # Reshape to (None, 28, 28, 1)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        # FFT layer\n",
    "        model.add(Lambda(lambda x: tf.signal.fft2d(tf.cast(x, tf.complex64))))\n",
    "        model.add(Lambda(lambda x: tf.abs(x)))  # Convert complex values to magnitude\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        #plot the model\n",
    "\n",
    "        freq_img = Input(shape=self.img_shape)\n",
    "        validity = model(freq_img)\n",
    "\n",
    "        return Model(freq_img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size, save_interval):\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        \n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # If at save interval, print and save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                print(f\"Epoch {epoch}: [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "                # Save generated images\n",
    "                self.save_images(epoch)\n",
    "\n",
    "    def save_images(self, epoch):\n",
    "        r, c = 5, 5  # Generate 5x5 grid of images\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, 2 * c)  # Double the number of columns for real and imaginary parts\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, 2 * j].imshow(gen_imgs[cnt, :, :, 0].real, cmap='gray')\n",
    "                axs[i, 2 * j].axis('off')\n",
    "                axs[i, 2 * j + 1].imshow(gen_imgs[cnt, :, :, 0].imag, cmap='gray')\n",
    "                axs[i, 2 * j + 1].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(f\"gan_imagesss/mnist_{epoch}.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the GAN class\n",
    "gan = GAN(img_rows=28, img_cols=28, channels=1, latent_dim=100)\n",
    "\n",
    "# Train the GAN\n",
    "#gan.train(epochs=10000, batch_size=128, save_interval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/home/kalyan/Pictures/screenshots/14-04-2023 11:51:38.png\"\n",
    "\n",
    "#take the image and convert it to frequency domain\n",
    "img = plt.imread(img_path)\n",
    "#take fft of the image\n",
    "img_fft = (np.fft.fft2(img))\n",
    "#take the real part of the image\n",
    "img_real = img_fft.real\n",
    "#plot the image\n",
    "plt.imshow(img_real)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model weights\n",
    "gan.generator.save_weights('gan_imagesss/generator_weights.h5')\n",
    "gan.discriminator.save_weights('gan_imagesss/discriminator_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model weights\n",
    "gen_weights = 'gan_imagesss/generator_weights.h5'\n",
    "disc_weights = 'gan_imagesss/discriminator_weights.h5'\n",
    "\n",
    "#load the model\n",
    "gan.generator.load_weights(gen_weights)\n",
    "gan.discriminator.load_weights(disc_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 2s 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input size must be at least 75x75; Received: input_shape=(10, 10, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m generated_images \u001b[39m=\u001b[39m gan\u001b[39m.\u001b[39mgenerator\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (X_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m100\u001b[39m)))\n\u001b[1;32m     52\u001b[0m \u001b[39m# Calculate FID between training images and generated images\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m fid \u001b[39m=\u001b[39m calculate_fid(X_train, generated_images)\n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFID: \u001b[39m\u001b[39m{\u001b[39;00mfid\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mcalculate_fid\u001b[0;34m(real_images, generated_images)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_fid\u001b[39m(real_images, generated_images):\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Load pre-trained Inception-v3 model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     inception_model \u001b[39m=\u001b[39m InceptionV3(include_top\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, pooling\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mavg\u001b[39;49m\u001b[39m'\u001b[39;49m, input_shape\u001b[39m=\u001b[39;49m(\u001b[39m10\u001b[39;49m, \u001b[39m10\u001b[39;49m, \u001b[39m3\u001b[39;49m))\n\u001b[1;32m     11\u001b[0m     \u001b[39m# Resize and preprocess real and generated images\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     real_images_resized \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mresize(real_images, (\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/applications/inception_v3.py:138\u001b[0m, in \u001b[0;36mInceptionV3\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIf using `weights` as `\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m` with `include_top` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    133\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas true, `classes` should be 1000; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived classes=\u001b[39m\u001b[39m{\u001b[39;00mclasses\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    137\u001b[0m \u001b[39m# Determine proper input shape\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m input_shape \u001b[39m=\u001b[39m imagenet_utils\u001b[39m.\u001b[39;49mobtain_input_shape(\n\u001b[1;32m    139\u001b[0m     input_shape,\n\u001b[1;32m    140\u001b[0m     default_size\u001b[39m=\u001b[39;49m\u001b[39m299\u001b[39;49m,\n\u001b[1;32m    141\u001b[0m     min_size\u001b[39m=\u001b[39;49m\u001b[39m75\u001b[39;49m,\n\u001b[1;32m    142\u001b[0m     data_format\u001b[39m=\u001b[39;49mbackend\u001b[39m.\u001b[39;49mimage_data_format(),\n\u001b[1;32m    143\u001b[0m     require_flatten\u001b[39m=\u001b[39;49minclude_top,\n\u001b[1;32m    144\u001b[0m     weights\u001b[39m=\u001b[39;49mweights,\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m input_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     img_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/applications/imagenet_utils.py:408\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    402\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mThe input must have 3 channels; Received \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m                 )\n\u001b[1;32m    405\u001b[0m             \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    406\u001b[0m                 input_shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m min_size\n\u001b[1;32m    407\u001b[0m             ) \u001b[39mor\u001b[39;00m (input_shape[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m min_size):\n\u001b[0;32m--> 408\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    409\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mInput size must be at least \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39mx\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39m; Received: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m                 )\n\u001b[1;32m    413\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m require_flatten:\n",
      "\u001b[0;31mValueError\u001b[0m: Input size must be at least 75x75; Received: input_shape=(10, 10, 3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.linalg import sqrtm\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "def calculate_fid(real_images, generated_images):\n",
    "    # Load pre-trained Inception-v3 model\n",
    "    inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(120, 120, 3))\n",
    "\n",
    "    # Resize and preprocess real and generated images\n",
    "    real_images_resized = tf.image.resize(real_images, (120, 120))\n",
    "    generated_images_resized = tf.image.resize(generated_images, (120, 120))\n",
    "    real_images_preprocessed = preprocess_input(real_images_resized)\n",
    "    generated_images_preprocessed = preprocess_input(generated_images_resized)\n",
    "\n",
    "    # Get feature representations of real and generated images\n",
    "    real_features = inception_model.predict(real_images_preprocessed)\n",
    "    generated_features = inception_model.predict(generated_images_preprocessed)\n",
    "\n",
    "    # Calculate mean and covariance of real and generated image features\n",
    "    real_mean = np.mean(real_features, axis=0)\n",
    "    real_covariance = np.cov(real_features, rowvar=False)\n",
    "    generated_mean = np.mean(generated_features, axis=0)\n",
    "    generated_covariance = np.cov(generated_features, rowvar=False)\n",
    "\n",
    "    # Calculate squared Euclidean distance between means\n",
    "    mean_diff = real_mean - generated_mean\n",
    "    mean_squared_distance = np.dot(mean_diff, mean_diff)\n",
    "\n",
    "    # Calculate trace of the product of covariances\n",
    "    cov_product = np.dot(real_covariance, generated_covariance)\n",
    "    cov_sqrt = sqrtm(cov_product)\n",
    "    trace_cov_sqrt = np.trace(cov_sqrt)\n",
    "\n",
    "    # Calculate Fréchet Inception Distance\n",
    "    fid = mean_squared_distance + np.trace(real_covariance) + np.trace(generated_covariance) - 2 * trace_cov_sqrt\n",
    "\n",
    "    return fid\n",
    "\n",
    "# Load the dataset\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Rescale -1 to 1 and preprocess images\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_train = preprocess_images(X_train)\n",
    "\n",
    "# Generate images using the generator network\n",
    "generated_images = gan.generator.predict(np.random.normal(0, 1, (X_train.shape[0], 100)))\n",
    "\n",
    "# Calculate FID between training images and generated images\n",
    "fid = calculate_fid(X_train, generated_images)\n",
    "print(f\"FID: {fid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
