{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "class KNeighborsClassifier():\n",
    "    \n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.neighbors = n_neighbors #this will initialize the number od neighbors to 5\n",
    "        \n",
    "    def fit():\n",
    "        pass\n",
    "    \n",
    "    def predict():\n",
    "        pass\n",
    "    \n",
    "    def euclidian_dist(self, point_1, point_2): #this function will calculate the euclidian distance between two points\n",
    "        dist = 0.0\n",
    "        for i in range(len(point_1) - 1):\n",
    "            dist += pow(point_1[i] - point_2[i], 2)\n",
    "        return np.sqrt(dist)\n",
    "    \n",
    "    def calc_distances(self, data, new_point): #this function will calculate the distance between the new point and all the points in the dataset\n",
    "        distances = []\n",
    "        neighbors = []\n",
    "        for i in data:\n",
    "            distances.append((i, self.euclidian_dist(new_point, i))) #this will append the distance between the new point and the point in the dataset\n",
    "        distances.sort(key=operator.itemgetter(1)) #this will sort the distances in ascending order\n",
    "        for i in range(self.neighbors):\n",
    "            neighbors.append(distances[i][0]) #this will append the neighbors to the neighbors list\n",
    "        return neighbors\n",
    "    \n",
    "    def find_majority(self, neighbors, train_X, train_y): #this function will find the majority of the neighbors\n",
    "        iter_y = [] \n",
    "        for i in neighbors:\n",
    "            iter_y.append(train_y[np.where(train_X == i)[0][0]]) #this will append the class of the neighbor to the iter_y list i.e if the neighbor is a cat then it will append cat to the list\n",
    "        return max(iter_y)\n",
    "    \n",
    "    def fit(self, train_X, train_y): #this will fit the model\n",
    "        set_of_classes = set(train_y) #this will find the unique classes in the dataset\n",
    "        self.classes = 0;\n",
    "        for i in set_of_classes: #this will count the number of classes in the dataset\n",
    "            self.classes += 1\n",
    "        self.X = train_X #this will store the training data\n",
    "        self.y = train_y #this will store the training labels\n",
    "        self.data_len = len(train_X) #this will store the length of the training data\n",
    "        \n",
    "    def predict(self, test_y):#this will predict the class of the test\n",
    "        y_pred = []  #this will store the predicted classes\n",
    "        neighbors = [] #this will store the neighbors\n",
    "        for i in test_y:\n",
    "            neighbors = self.calc_distances(self.X, i) #this will find the neighbors\n",
    "            y_pred.append(self.find_majority(neighbors, self.X, self.y)) #this will find the majority of the neighbors\n",
    "        return y_pred #and we will return the predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans():\n",
    "    \n",
    "    def __init__(self, n_clusters=5):\n",
    "        self.clusters = n_clusters\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        self.data_len = len(X)\n",
    "        self.centroids = []\n",
    "        for i in range(self.clusters):\n",
    "            self.centroids.append(X[i])\n",
    "        self.centroids = np.array(self.centroids)\n",
    "        self.clusters = np.zeros(self.data_len)\n",
    "\n",
    "    def euclidian_dist(self, point_1, point_2):\n",
    "        dist = 0.0\n",
    "        for i in range(len(point_1)):\n",
    "            dist += pow(point_1[i] - point_2[i], 2)\n",
    "        return np.sqrt(dist)\n",
    "\n",
    "    def calc_distances(self, data, new_point):\n",
    "        distances = []\n",
    "        for i in data:\n",
    "            distances.append(self.euclidian_dist(new_point, i))\n",
    "        return distances\n",
    "\n",
    "    def find_nearest_centroid(self, new_point):\n",
    "        distances = self.calc_distances(self.centroids, new_point)\n",
    "        return np.argmin(distances)\n",
    "\n",
    "    def update_centroids(self):\n",
    "        for i in range(self.clusters):\n",
    "            points = [self.X[j] for j in range(len(self.X)) if self.clusters[j] == i]\n",
    "            self.centroids[i] = np.mean(points, axis=0)\n",
    "            \n",
    "    def predict(self, test_X):\n",
    "        y_pred = []\n",
    "        for i in test_X:\n",
    "            y_pred.append(self.find_nearest_centroid(i))\n",
    "        return y_pred\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training cat images: 25\n",
      "Total training dog images: 25\n",
      "Total test cat images: 50\n",
      "Total test dog images: 50\n"
     ]
    }
   ],
   "source": [
    "#importing the data\n",
    "data_dir = '/home/kalyan/DataSets/DogsandCats/random_images'\n",
    "train_dir = os.path.join(data_dir, 'training_set/training_set/')\n",
    "test_dir = os.path.join(data_dir, 'test_set/test_set')\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "\n",
    "\n",
    "#checking the number of images in each folder\n",
    "print('Total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('Total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "print('Total test cat images:', len(os.listdir(test_cats_dir)))\n",
    "print('Total test dog images:', len(os.listdir(test_dogs_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking svd of the images and to truncatate first 10 features of the images as it is most contributing to the images\n",
    "'''\n",
    "Input : An image and number of features to be truncated\n",
    "Output : Truncated US matrix\n",
    "'''\n",
    "def svd_truncate(img, n):\n",
    "    #converting images to grayscale\n",
    "    img = img.mean(axis=2) # convert to grayscale\n",
    "    U, s, V = np.linalg.svd(img)\n",
    "    U = U[:, :n]\n",
    "    s = s[:n]\n",
    "    us = np.dot(U, np.diag(s))\n",
    "    return us\n",
    "#getting svd of all images\n",
    "'''\n",
    "Input : Directory of the images and number of features to be truncated\n",
    "Output : List of truncated US matrix\n",
    "'''\n",
    "def get_svd_images(img_dir, n):\n",
    "    img_files = os.listdir(img_dir)\n",
    "    img_files = [os.path.join(img_dir, f) for f in img_files]\n",
    "    #img_files = [plt.imread(f) for f in img_files]\n",
    "    # read images from file, resize them into 100x100, store in single array\n",
    "    img_files = [cv2.resize(plt.imread(f), (200, 200)) for f in img_files]\n",
    "    svd_images = [svd_truncate(img, n) for img in img_files]\n",
    "    return svd_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 200, 5)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train Images\n",
    "Input : Directory of the images and number of features to be truncated\n",
    "Output : Truncated US matrix of cats and dogs\n",
    "'''\n",
    "cat_images = get_svd_images(train_cats_dir, 5)\n",
    "cat_images = np.array(cat_images)\n",
    "dog_images = get_svd_images(train_dogs_dir, 5)\n",
    "dog_images = np.array(dog_images)\n",
    "#concatenating the images\n",
    "train_images = np.concatenate((cat_images, dog_images), axis=0)\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "#creating labels for the train images\n",
    "train_labels = np.concatenate((np.zeros(len(cat_images)), np.ones(len(dog_images))), axis=0)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 200, 5)\n",
      "(50, 200, 5)\n",
      "(100, 200, 5)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Images\n",
    "Input : Directory of the images and number of features to be truncated\n",
    "Output : Truncated US matrix of cats and dogs\n",
    "'''\n",
    "test_cat_images = get_svd_images(test_cats_dir, 5)\n",
    "test_cat_images = np.array(test_cat_images)\n",
    "print(test_cat_images.shape)\n",
    "#test for dogs\n",
    "test_dog_images = get_svd_images(test_dogs_dir, 5)\n",
    "test_dog_images = np.array(test_dog_images)\n",
    "print(test_dog_images.shape)\n",
    "#concatenating the test images\n",
    "test_images = np.concatenate((test_cat_images, test_dog_images), axis=0)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1000)\n",
      "(100, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_image_1d = np.array([img.flatten() for img in train_images])\n",
    "test_image_1d = np.array([img.flatten() for img in test_images])\n",
    "print(train_image_1d.shape)\n",
    "print(test_image_1d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "#creating labels for the test images\n",
    "test_labels = np.concatenate((np.zeros(len(test_cat_images)), np.ones(len(test_dog_images))), axis=0)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_image_1d,train_labels)\n",
    "y_pred = knn.predict(test_image_1d)\n",
    "print(accuracy_score(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_image_1d, train_labels)\n",
    "y_pred = knn.predict(test_image_1d)\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.14      0.21        50\n",
      "         1.0       0.48      0.80      0.60        50\n",
      "\n",
      "    accuracy                           0.47       100\n",
      "   macro avg       0.45      0.47      0.41       100\n",
      "weighted avg       0.45      0.47      0.41       100\n",
      "\n",
      "[[ 7 43]\n",
      " [10 40]]\n"
     ]
    }
   ],
   "source": [
    "#get f1 score,precision recall and confusion matrix\n",
    "#for knn\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels, y_pred))\n",
    "print(confusion_matrix(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43\n"
     ]
    }
   ],
   "source": [
    "kmenas= Kmeans(5)\n",
    "kmenas.fit(train_image_1d)\n",
    "y_pred = kmenas.predict(test_image_1d)\n",
    "print(accuracy_score(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.54      0.53        50\n",
      "         1.0       0.59      0.32      0.42        50\n",
      "         2.0       0.00      0.00      0.00         0\n",
      "         3.0       0.00      0.00      0.00         0\n",
      "         4.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.43       100\n",
      "   macro avg       0.22      0.17      0.19       100\n",
      "weighted avg       0.56      0.43      0.47       100\n",
      "\n",
      "[[27 11  2  4  6]\n",
      " [25 16  2  1  6]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalyan/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kalyan/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kalyan/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#get f1 score,precision recall and confusion matrix\n",
    "#for knn\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels, y_pred))\n",
    "print(confusion_matrix(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.07s/trial, best loss: 0.8]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.33s/trial, best loss: 0.6]\n",
      "100%|██████████| 4/4 [00:00<00:00,  6.12trial/s, best loss: 0.5]\n",
      "100%|██████████| 5/5 [00:00<00:00,  2.20trial/s, best loss: 0.5]\n",
      "100%|██████████| 6/6 [00:00<00:00,  3.63trial/s, best loss: 0.5]\n",
      "100%|██████████| 7/7 [00:09<00:00,  9.08s/trial, best loss: 0.5]\n",
      "100%|██████████| 8/8 [00:00<00:00, 11.73trial/s, best loss: 0.5]\n",
      "100%|██████████| 9/9 [00:02<00:00,  2.70s/trial, best loss: 0.5]\n",
      "100%|██████████| 10/10 [00:06<00:00,  6.79s/trial, best loss: 0.5]\n",
      "100%|██████████| 11/11 [00:02<00:00,  2.32s/trial, best loss: 0.5]\n",
      "100%|██████████| 12/12 [00:03<00:00,  3.84s/trial, best loss: 0.5]\n",
      "100%|██████████| 13/13 [00:00<00:00,  5.62trial/s, best loss: 0.5]\n",
      "100%|██████████| 14/14 [00:00<00:00,  4.97trial/s, best loss: 0.5]\n",
      "100%|██████████| 15/15 [00:00<00:00,  5.46trial/s, best loss: 0.5]\n",
      "100%|██████████| 16/16 [00:00<00:00,  3.26trial/s, best loss: 0.5]\n",
      "100%|██████████| 17/17 [00:00<00:00,  7.94trial/s, best loss: 0.5]\n",
      "100%|██████████| 18/18 [00:00<00:00,  7.85trial/s, best loss: 0.5]\n",
      "100%|██████████| 19/19 [00:01<00:00,  1.47s/trial, best loss: 0.5]\n",
      "100%|██████████| 20/20 [00:00<00:00,  1.29trial/s, best loss: 0.5]\n",
      "100%|██████████| 21/21 [00:00<00:00,  7.09trial/s, best loss: 0.5]\n",
      "100%|██████████| 22/22 [00:00<00:00,  4.31trial/s, best loss: 0.5]\n",
      "100%|██████████| 23/23 [00:00<00:00,  6.92trial/s, best loss: 0.5]\n",
      " 96%|█████████▌| 23/24 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalyan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning,\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00,  2.10trial/s, best loss: 0.5]\n",
      "100%|██████████| 25/25 [00:00<00:00,  7.46trial/s, best loss: 0.30000000000000004]\n",
      "100%|██████████| 26/26 [00:00<00:00,  7.25trial/s, best loss: 0.30000000000000004]\n",
      "100%|██████████| 27/27 [00:00<00:00,  4.34trial/s, best loss: 0.30000000000000004]\n",
      "100%|██████████| 28/28 [00:00<00:00,  7.87trial/s, best loss: 0.30000000000000004]\n",
      "100%|██████████| 29/29 [00:00<00:00,  7.02trial/s, best loss: 0.30000000000000004]\n",
      "100%|██████████| 30/30 [00:00<00:00,  4.46trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 31/31 [00:00<00:00,  7.90trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 32/32 [00:00<00:00,  4.52trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 33/33 [00:00<00:00,  7.39trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 34/34 [00:00<00:00,  7.41trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 35/35 [00:00<00:00,  3.56trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 36/36 [00:00<00:00,  7.01trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 37/37 [00:00<00:00,  4.11trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 38/38 [00:00<00:00,  7.25trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 39/39 [00:00<00:00,  7.22trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 40/40 [00:00<00:00,  5.00trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 41/41 [00:00<00:00,  7.32trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 42/42 [00:00<00:00,  3.46trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 43/43 [00:00<00:00,  6.67trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 44/44 [00:00<00:00,  7.71trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 45/45 [00:00<00:00,  6.90trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 46/46 [00:00<00:00,  7.26trial/s, best loss: 0.19999999999999996]\n",
      " 98%|█████████▊| 46/47 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalyan/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/_weight_boosting.py:657: RuntimeWarning: overflow encountered in exp\n",
      "  + estimator_weight * incorrect * (sample_weight > 0)\n",
      "\n",
      "/home/kalyan/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/_weight_boosting.py:486: UserWarning: Sample weights have reached infinite values, at iteration 25, causing overflow. Iterations stopped. Try lowering the learning rate.\n",
      "  return super().fit(X, y, sample_weight)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00,  3.28trial/s, best loss: 0.19999999999999996]\n",
      "100%|██████████| 48/48 [02:00<00:00, 120.22s/trial, best loss: 0.19999999999999996]\n",
      "100%|██████████| 49/49 [00:00<00:00,  6.53trial/s, best loss: 0.0]\n",
      "100%|██████████| 50/50 [00:00<00:00,  6.55trial/s, best loss: 0.0]\n",
      "100%|██████████| 51/51 [00:00<00:00,  7.04trial/s, best loss: 0.0]\n",
      "100%|██████████| 52/52 [00:03<00:00,  3.13s/trial, best loss: 0.0]\n",
      "100%|██████████| 53/53 [00:03<00:00,  3.93s/trial, best loss: 0.0]\n",
      "100%|██████████| 54/54 [00:00<00:00,  1.32trial/s, best loss: 0.0]\n",
      "100%|██████████| 55/55 [00:00<00:00,  1.04trial/s, best loss: 0.0]\n",
      "100%|██████████| 56/56 [00:01<00:00,  1.37s/trial, best loss: 0.0]\n",
      "100%|██████████| 57/57 [00:00<00:00,  1.04trial/s, best loss: 0.0]\n",
      "100%|██████████| 58/58 [00:00<00:00,  8.29trial/s, best loss: 0.0]\n",
      "100%|██████████| 59/59 [00:00<00:00,  3.92trial/s, best loss: 0.0]\n",
      "100%|██████████| 60/60 [00:00<00:00,  4.06trial/s, best loss: 0.0]\n",
      "100%|██████████| 61/61 [00:00<00:00,  3.62trial/s, best loss: 0.0]\n",
      "100%|██████████| 62/62 [00:03<00:00,  3.25s/trial, best loss: 0.0]\n",
      "100%|██████████| 63/63 [00:01<00:00,  1.27s/trial, best loss: 0.0]\n",
      "100%|██████████| 64/64 [00:03<00:00,  3.86s/trial, best loss: 0.0]\n",
      "100%|██████████| 65/65 [00:00<00:00,  6.69trial/s, best loss: 0.0]\n",
      "100%|██████████| 66/66 [00:01<00:00,  1.67s/trial, best loss: 0.0]\n",
      "100%|██████████| 67/67 [00:00<00:00,  2.02trial/s, best loss: 0.0]\n",
      "100%|██████████| 68/68 [00:00<00:00,  3.81trial/s, best loss: 0.0]\n",
      "100%|██████████| 69/69 [00:00<00:00,  2.13trial/s, best loss: 0.0]\n",
      "100%|██████████| 70/70 [00:11<00:00, 11.97s/trial, best loss: 0.0]\n",
      "100%|██████████| 71/71 [00:01<00:00,  1.77s/trial, best loss: 0.0]\n",
      "100%|██████████| 72/72 [00:01<00:00,  1.12s/trial, best loss: 0.0]\n",
      "100%|██████████| 73/73 [00:00<00:00,  7.54trial/s, best loss: 0.0]\n",
      "100%|██████████| 74/74 [00:00<00:00,  3.34trial/s, best loss: 0.0]\n",
      "100%|██████████| 75/75 [00:00<00:00,  6.08trial/s, best loss: 0.0]\n",
      "100%|██████████| 76/76 [00:04<00:00,  4.34s/trial, best loss: 0.0]\n",
      "100%|██████████| 77/77 [00:00<00:00,  1.26trial/s, best loss: 0.0]\n",
      "100%|██████████| 78/78 [00:00<00:00,  1.67trial/s, best loss: 0.0]\n",
      "100%|██████████| 79/79 [00:04<00:00,  4.50s/trial, best loss: 0.0]\n",
      "100%|██████████| 80/80 [00:00<00:00,  4.93trial/s, best loss: 0.0]\n",
      "100%|██████████| 81/81 [00:04<00:00,  4.09s/trial, best loss: 0.0]\n",
      "100%|██████████| 82/82 [00:00<00:00,  4.06trial/s, best loss: 0.0]\n",
      "100%|██████████| 83/83 [00:00<00:00,  2.14trial/s, best loss: 0.0]\n",
      "100%|██████████| 84/84 [00:00<00:00,  5.80trial/s, best loss: 0.0]\n",
      "100%|██████████| 85/85 [00:03<00:00,  3.38s/trial, best loss: 0.0]\n",
      "100%|██████████| 86/86 [00:00<00:00,  5.65trial/s, best loss: 0.0]\n",
      "100%|██████████| 87/87 [00:01<00:00,  1.54s/trial, best loss: 0.0]\n",
      "100%|██████████| 88/88 [00:00<00:00,  1.27trial/s, best loss: 0.0]\n",
      "100%|██████████| 89/89 [00:03<00:00,  3.45s/trial, best loss: 0.0]\n",
      "100%|██████████| 90/90 [00:13<00:00, 13.67s/trial, best loss: 0.0]\n",
      "100%|██████████| 91/91 [00:00<00:00,  4.19trial/s, best loss: 0.0]\n",
      "100%|██████████| 92/92 [00:00<00:00,  7.03trial/s, best loss: 0.0]\n",
      "100%|██████████| 93/93 [00:00<00:00,  3.85trial/s, best loss: 0.0]\n",
      "100%|██████████| 94/94 [00:00<00:00,  5.53trial/s, best loss: 0.0]\n",
      "100%|██████████| 95/95 [00:00<00:00,  4.08trial/s, best loss: 0.0]\n",
      "100%|██████████| 96/96 [00:00<00:00,  5.60trial/s, best loss: 0.0]\n",
      "100%|██████████| 97/97 [00:00<00:00,  3.45trial/s, best loss: 0.0]\n",
      "100%|██████████| 98/98 [00:00<00:00,  3.39trial/s, best loss: 0.0]\n",
      "100%|██████████| 99/99 [00:00<00:00,  6.18trial/s, best loss: 0.0]\n",
      "100%|██████████| 100/100 [00:00<00:00,  1.89trial/s, best loss: 0.0]\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "#using HyperoptEstimator\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\n",
    "from hyperopt import tpe\n",
    "import numpy as np\n",
    "\n",
    "estim = HyperoptEstimator(classifier=any_classifier(\"my_clf\"),\n",
    "                              preprocessing=any_preprocessing(\"my_pre\"),\n",
    "                              algo=tpe.suggest,\n",
    "                              max_evals=100,\n",
    "                              trial_timeout=120)\n",
    "\n",
    "estim.fit(train_image_1d, train_labels)\n",
    "print(estim.score(test_image_1d, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learner': SVC(C=0.563836014184182, coef0=0.6591710063188843,\n",
      "    decision_function_shape='ovo', degree=2, gamma='auto', kernel='poly',\n",
      "    random_state=4, tol=3.721312395841434e-05), 'preprocs': (PCA(n_components=40, whiten=True),), 'ex_preprocs': ()}\n"
     ]
    }
   ],
   "source": [
    "print(estim.best_model())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18435721447aaa7b29792b509b4b765c4d69b94478d2c6d7cbe357a9dc2f4d62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
