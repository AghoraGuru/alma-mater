{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What it is \n",
    "* Logistic Regression is a statistical method used to fit a regression model when the response variable is binary. In logistic regression, the goal is to find the best coefficients, denoted by $\\beta_0, \\beta_1, ..., \\beta_p$, that will separate the positive instances from the negative instances in the training set.\n",
    "\n",
    "* The logistic function, also known as the sigmoid function, is used to model the probability of the positive class given the feature values:\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)}}$$\n",
    "Where $x$ are the feature values and $\\beta_0, \\beta_1, ..., \\beta_p$ are the coefficients.\n",
    "\n",
    "* To find the best coefficients, we need to maximize the likelihood of the observed data. The likelihood is defined as the probability of the observed data given the model. The negative log-likelihood is used as a cost function to minimize during the optimization process.\n",
    "\n",
    "* The cost function for logistic regression is given by:\n",
    "$$J(\\beta) = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i log(P(y_i=1|x_i)) + (1-y_i)log(1-P(y_i=1|x_i))]$$\n",
    "\n",
    "Where $n$ is the number of instances in the training set.\n",
    "\n",
    "* This cost function is optimized using optimization algorithms like gradient descent, Newton-Raphson method or conjugate gradient optimization etc.\n",
    "\n",
    "* Once the optimal values of coefficients are found, the model can be used to predict the probability of the positive class for new instances. A threshold is chosen to convert the predicted probability into a binary output.\n",
    "\n",
    "* In summary, Logistic Regression is a supervised learning algorithm used to classify binary data by fitting a logistic function to the input features and the output variable, and then maximizing the likelihood of the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1\n",
    "* Logistic Regression is a statistical method that we use to fit a regression model when the response variable is binary. L1 regularization, also known as Lasso regularization, is a method to avoid overfitting by adding a penalty term to the cost function. The penalty term is the absolute value of the coefficients multiplied by a hyperparameter, lambda. This has the effect of shrinking the coefficients towards zero, which in turn can lead to some of the features being completely ignored by the model (i.e., the coefficients become exactly zero).\n",
    "\n",
    "* The L1 regularization term causes some coefficients to become exactly zero. This can be useful when we have a high number of features and we want to select a subset for our model. This method is known as feature selection.\n",
    "\n",
    "* Advantage of L1 regularization is that it is computationally efficient, and can be useful when we have a large number of features and we want to select a subset for our model.\n",
    "\n",
    "* A disadvantage of L1 regularization is that it is not differentiable, which makes it more difficult to optimize the cost function using gradient-based optimization algorithms. Additionally, L1 regularization can lead to unstable solutions and yield models that are difficult to interpret.\n",
    "\n",
    "* Overall, L1 regularization is a useful technique to prevent overfitting and to select a subset of features for a logistic regression model. However, it should be used with caution, as it can lead to unstable solutions and models that are difficult to interpret.\n",
    "* The goal of logistic regression is to find the best coefficients $\\beta_0, \\beta_1, ..., \\beta_p$ that will separate the positive instances from the negative instances in the training set.\n",
    "\n",
    "* The logistic function is used to model the probability of the positive class given the feature values:\n",
    "$$P(y=1|x) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)}}$$\n",
    "Where $x$ is the feature vector, $\\beta_0, \\beta_1, ..., \\beta_p$ are the coefficients and $e$ is the base of the natural logarithm.\n",
    "\n",
    "* To find the best coefficients, we need to maximize the likelihood of the observed data. The likelihood is defined as the probability of the observed data given the model. The negative log-likelihood is used as a cost function to minimize during the optimization process.\n",
    "\n",
    "* The cost function for logistic regression is given by:\n",
    "$$J(\\beta) = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i log(P(y_i=1|x_i)) + (1-y_i)log(1-P(y_i=1|x_i))]$$\n",
    "Where $n$ is the number of instances in the training set.\n",
    "\n",
    "* With L1 regularization, we add a penalty term to the cost function, which is the absolute value of the coefficients multiplied by a hyperparameter lambda. This has the effect of shrinking the coefficients towards zero. The cost function for logistic regression with L1 regularization is given by:\n",
    "$$J(\\beta) = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i log(P(y_i=1|x_i)) + (1-y_i)log(1-P(y_i=1|x_i))] + \\lambda\\sum_{j=1}^{p}|\\beta_j|$$\n",
    "Where $\\lambda$ is the regularization parameter, and it controls the strength of the regularization. A higher value of $\\lambda$ will result in smaller coefficients.\n",
    "\n",
    "* It is important to note that this cost function is not differentiable at $\\beta_j=0$, so optimization algorithm like gradient descent can not be used directly. Instead, sub-gradient descent is used to optimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2\n",
    "* L2 regularization, also known as Ridge regularization, is a method to avoid overfitting by adding a penalty term to the cost function. The penalty term is the sum of the squares of the coefficients multiplied by a hyperparameter, lambda. This has the effect of shrinking the coefficients towards zero, which in turn can lead to a simpler model.\n",
    "\n",
    "* The L2 regularization term causes the coefficients to be smaller, and it does not force any coefficients to be exactly zero. This can be useful when we want to balance the trade-off between a simpler model and a model that fits the data well.\n",
    "\n",
    "* The cost function for logistic regression with L2 regularization is given by:\n",
    "$$J(\\beta) = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i log(P(y_i=1|x_i)) + (1-y_i)log(1-P(y_i=1|x_i))] + \\frac{\\lambda}{2}\\sum_{j=1}^{p}\\beta_j^2$$ \n",
    "Where $\\lambda$ is the regularization parameter, and it controls the strength of the regularization. A higher value of $\\lambda$ will result in smaller coefficients.\n",
    "* It is important to note that L2 regularization is differentiable so it can be optimized using optimization algorithms like gradient descent.\n",
    "\n",
    "* As L2 regularization causes the coefficients to be smaller, it can help reduce the variance of the model and prevent overfitting. However, it may not be as effective as L1 regularization in selecting a subset of features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforwarding in Logistic Regression\n",
    "In logistic regression, feedforward is the process of calculating the predicted output from the input features and the coefficients of the model.\n",
    "\n",
    "The feedforward process starts by taking the input feature values and multiplying them with the corresponding coefficients, then it sums them up and applies the logistic function to the result.\n",
    "\n",
    "The logistic function, also known as the sigmoid function, is defined as:\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "Where $z$ is the input to the function.\n",
    "\n",
    "In logistic regression, we compute the input to the logistic function as:\n",
    "$$z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p$$\n",
    "Where $\\beta_0, \\beta_1, ..., \\beta_p$ are the coefficients of the model and $x_1, x_2, ..., x_p$ are the features.\n",
    "The predicted output, also known as the hypothesis, is then given by the logistic function applied to the input:\n",
    "$$\\hat{y} = \\sigma(z) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)}}$$\n",
    "It is important to note that the logistic function outputs a probability of the positive class, so a threshold is chosen to convert the predicted probability into a binary output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Logistic Regression\n",
    "* Backpropagation is an algorithm used to update the coefficients of a model during the training process. It is commonly used in neural networks but can also be applied to logistic regression. Backpropagation works by calculating the gradient of the cost function with respect to the coefficients and using it to update the coefficients in the opposite direction of the gradient.\n",
    "\n",
    "* In logistic regression, the cost function is the negative log-likelihood of the observed data given the model. The gradient of the cost function with respect to the coefficients is given by:\n",
    "$$\\frac{\\partial J}{\\partial \\beta_j} = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i - \\hat{y_i}]x_j$$\n",
    "Where $\\hat{y_i}$ is the predicted probability of the positive class for the i-th instance in the training set, $y_i$ is the actual label, $x_j$ is the j-th feature value for the i-th instance, and $n$ is the number of instances in the training set.\n",
    "\n",
    "* The coefficients are then updated using the gradient:\n",
    "$$\\beta_j = \\beta_j - \\alpha\\frac{\\partial J}{\\partial \\beta_j}$$\n",
    "Where $\\alpha$ is the learning rate, which controls the step size of the updates.\n",
    "\n",
    "* This process is repeated for a number of iterations until the cost function converges to a minimum.\n",
    "\n",
    "* It's important to note that Backpropagation is a supervised learning algorithm, which means it uses labeled data to learn the model's parameters. Backpropagation is mainly used in neural networks but can also be used in logistic regression to optimize the cost function with respect to the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing logistic regression \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score ,f1_score, precision_score, recall_score, confusion_matrix , classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset from sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df.head()\n",
    "\n",
    "#splitting data into train and test\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will be doing grid search to find the best hyperparameters for logistic regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score ,f1_score, precision_score, recall_score, confusion_matrix , classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting datapoints\n",
    "plt.scatter(X_train['mean radius'], X_train['mean texture'], c=y_train, cmap='rainbow')\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, max_iter=1000,penalty='l1',solver='liblinear')\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100,200]}\n",
    "grid = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "#putting it all in a df\n",
    "df = pd.DataFrame({'C':param_grid['C'],'Accuracy':grid.cv_results_['mean_test_score']})\n",
    "df = df.set_index('C')\n",
    "df = df.sort_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting classification report\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000,penalty='l2')\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100,200]}\n",
    "grid = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "#putting it all in a df\n",
    "dff = pd.DataFrame({'C':param_grid['C'],'Accuracy':grid.cv_results_['mean_test_score']})\n",
    "dff = dff.set_index('C')\n",
    "dff = dff.sort_index()\n",
    "display(dff)\n",
    "#plotting importance of C\n",
    "plt.plot(dff.index,dff['Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy with l1 penalty')\n",
    "plt.title('Accuracy vs C')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C value should be from 8 to 15 with 0.1 step\n",
    "C = np.arange(1,100,0.1)\n",
    "l1accuracy = []\n",
    "l2accuracy = []\n",
    "for c in C:\n",
    "    l1 = LogisticRegression(random_state=0, max_iter=1000,penalty='l1',solver='liblinear',C=c)\n",
    "    l1.fit(X_train,y_train)\n",
    "    l1y_pred = l1.predict(X_test)\n",
    "    l1accuracy.append(accuracy_score(y_test,l1y_pred))\n",
    "    l2 = LogisticRegression(random_state=0, max_iter=1000,penalty='l2',C=c)\n",
    "    l2.fit(X_train,y_train)\n",
    "    l2y_pred = l2.predict(X_test)\n",
    "    l2accuracy.append(accuracy_score(y_test,l2y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3</th>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.4</th>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.5</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.6</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.7</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.8</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.9</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            l1       l2\n",
       "C                      \n",
       "1.0   0.956140  0.95614\n",
       "1.1   0.956140  0.95614\n",
       "1.2   0.956140  0.95614\n",
       "1.3   0.956140  0.95614\n",
       "1.4   0.956140  0.95614\n",
       "...        ...      ...\n",
       "99.5  0.982456  0.95614\n",
       "99.6  0.982456  0.95614\n",
       "99.7  0.982456  0.95614\n",
       "99.8  0.982456  0.95614\n",
       "99.9  0.982456  0.95614\n",
       "\n",
       "[990 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make a df with l1accuracy and l2accuracy as columns\n",
    "df = pd.DataFrame({'C':C,'l1':l1accuracy,'l2':l2accuracy})\n",
    "df = df.set_index('C')\n",
    "df = df.sort_index()\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.95614</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <td>0.95614</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>0.95614</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3</th>\n",
       "      <td>0.95614</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.4</th>\n",
       "      <td>0.95614</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          l1       l2\n",
       "C                    \n",
       "1.0  0.95614  0.95614\n",
       "1.1  0.95614  0.95614\n",
       "1.2  0.95614  0.95614\n",
       "1.3  0.95614  0.95614\n",
       "1.4  0.95614  0.95614"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transpose()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting l1accuracy and l2accuracy vs C without using df\n",
    "plt.plot(C,l1accuracy,label='l1')\n",
    "plt.plot(C,l2accuracy,label = 'l2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
